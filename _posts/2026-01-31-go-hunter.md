---
layout: post
title: "Go-Hunter: Multi-Cloud Attack Surface Management"
date: 2026-01-31
category: projects
---

Cloud security platform for discovering and monitoring assets across AWS, GCP, Azure, Cloudflare, and DigitalOcean.

![Dashboard](/assets/images/projects/go-hunter-dashboard.png)

Organizations running multi-cloud infrastructure face a fundamental visibility problem. Development teams spin up VMs, storage buckets, DNS records, and load balancers across different providers without a unified inventory. Each resource is a potential attack vector, and security teams often discover exposed assets only after an incident. Go-Hunter addresses this by continuously discovering cloud resources, scanning for misconfigurations, and tracking configuration drift across five major cloud providers.

The multi-cloud integration challenge is more than just API wrappers. Each provider has different authentication models, rate limits, and resource hierarchies. AWS uses IAM credentials with complex permission policies. GCP requires service accounts with specific role bindings. Azure needs app registrations. Cloudflare and DigitalOcean use API tokens. Rather than forcing teams to manage separate scanning tools, Go-Hunter normalizes these differences behind a single interface. All credentials are encrypted at rest using the age library before storage in PostgreSQL. Decryption happens in-memory only when workers need to execute scans, and audit logs track every credential access.

Rate limiting proved unexpectedly complex. AWS throttles API calls aggressively, especially for discovery operations that enumerate resources across multiple regions. The solution uses per-provider rate limiters with exponential backoff. Scans take longer but don't trigger throttling errors that would leave blind spots in asset inventory. Testing revealed that patient, consistent scanning beats aggressive parallelization for reliability.

## Architecture

The system separates concerns cleanly. A stateless API server built with Chi handles HTTP requests and serves the dashboard. All state lives in PostgreSQL, making horizontal scaling straightforward. Background workers using Asynq pull scan jobs from a Redis queue, execute cloud API calls, and write discovered assets back to the database. Adding capacity means spinning up more worker processes. No coordination required.

The data model reflects multi-tenant SaaS from the start. Organizations own Users and Credentials. Scans produce Assets. Assets accumulate Findings over time. Every database query includes organization ID filtering to prevent cross-tenant data leakage. I tested this isolation extensively because multi-tenant bugs are catastrophic. Row-level filtering with indexed foreign keys keeps queries fast even as data grows.

Performance optimization focused on database bottlenecks. Initial discovery scans were slow because the code inserted assets individually. Switching to batch inserts and adding composite indexes on organization ID and asset type improved throughput from roughly 50 assets per minute to over 1,000. The worker pool pattern with goroutines and context-aware cancellation keeps scanning responsive even when processing large cloud environments.

Scanning capabilities cover the attack surface systematically. Asset discovery enumerates all resources by calling provider APIs on schedule or on-demand. Port scanning uses TCP connect probes on common ports to identify exposed services. The code includes banner grabbing for service version detection. S3 bucket checks test for public read and write access, catching the misconfiguration that leads to data leaks. Drift detection hashes asset state with SHA256 and alerts when configurations change unexpectedly, which helps catch unauthorized modifications or policy violations.

Security design went beyond just encrypting credentials. Multi-tenant isolation uses database-level filtering enforced at the query layer. Audit logging captures credential access, scan execution, and configuration changes. The worker architecture means credentials never touch the API server. Workers decrypt them in isolated processes, use them for scanning, and discard them from memory. This compartmentalization limits the blast radius if a component is compromised.

The HTMX dashboard works well for basic operations but shows its limits with complex filtering and data visualization. Building it was fast, which mattered for shipping quickly. A proper frontend framework would enable richer interactions for power users. That trade-off made sense for an initial version. I would also add vulnerability correlation earlier in the next iteration. The scanner finds open ports and misconfigured storage, but it does not cross-reference discovered services with CVE databases. Connecting asset inventory to known vulnerabilities would make findings immediately actionable.

Working on Go-Hunter taught me that cloud security tooling is fundamentally about managing API complexity and scale. The concurrent scanning architecture with worker pools and context cancellation proved robust under load. Multi-tenant isolation requires constant vigilance in both design and testing. Rate limiting is not an afterthought. It is a first-class architectural concern when integrating with third-party APIs. These patterns apply beyond security scanning to any system that orchestrates work across multiple cloud providers.

---

[View on GitHub](https://github.com/HueCodes/Go-Hunter)
